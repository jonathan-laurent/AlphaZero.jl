"""
    TrainUtilities

This module provides utilities to facilitate the training of AlphaZero agents.

The utilities focus on setting up environments, configuring MCTS algorithms, neural network
training, and logging. It provides support for two types of MCTS algorithms: Traditional
AlphaZero MCTS and Gumbel-MCTS.

## Structs

- `TrainConfig`: A structure to hold all the training parameters including environment
   configuration, MCTS settings, neural network training variables, logging, and evaluation
   settings.

- `TrainExecutionTimes`: A structure for storing execution times of significant parts of
   AlphaZero's training loop.

## Functions

- `init_envs`: Function to initialize environments for training based on a `TrainConfig`.

- `init_mcts_config`: Function to initialize the configuration object for MCTS algorithms,
   based on the type of MCTS algorithm specified in `TrainConfig`.

- `save_nn`: Function to save the neural network model to disk.

- `print_execution_times`: Function to print the execution times stored in a
   `TrainExecutionTimes` object.

## Example

```julia
using TrainUtilities

config = TrainConfig(
    EnvCls = MyEnv,  # must folow BatchedEnvs interface
    env_kwargs = Dict(:size => 5),
    num_envs = 20
)

envs, steps_counter = init_envs(config, 20, CPU())
mcts_config = init_mcts_config(CPU(), myNN, config)
```
"""
module TrainUtilities

using Base: @kwdef
using Flux
using JLD2

using ...BatchedMctsUtilities
using ...EnvOracles
using ...Network
using ...Util.Devices


export TrainConfig, TrainExecutionTimes
export init_envs, init_mcts_config, save_nn, get_train_timestamps, print_execution_times


"""
    TrainConfig(; kwargs...)

Configuration for training an AlphaZero agent.

# Fields

## Environment variables
- `EnvCls::Type`: The type of the environment to be used. The environment structure should
   follow the `BatchedEnvs` interface.
- `env_kwargs::Dict`: Keyword arguments to be passed when creating the environment instance.
- `num_envs::Int`: The number of environments to be used for parallel simulations.

Note: Environment instances are initialized like so: `env = EnvCls(; env_kwargs...)`

## Common MCTS variables
- `use_gumbel_mcts::Bool = true`: Whether to use Gumbel-MCTS instead of traditional MCTS.
- `num_simulations::Int = 64`: Number of MCTS simulations per move.

Note: The two types of implemented MCTS algorithms can be found in the following links:
- [Gumbel-MCTS](https://openreview.net/pdf?id=bERaNdoegnO)
- [AlphaZero-MCTS](https://arxiv.org/pdf/1712.01815.pdf)

## Gumbel MCTS variables
- `num_considered_actions::Int = 9`: Number of actions considered for selection in
   Gumbel-MCTS.
- `mcts_value_scale::Float32 = 0.1f0`: `c_scale` parameter for Gumbel-MCTS.
- `mcts_max_visit_init::Int = 50`: `c_visit` parameter for Gumbel-MCTS.

## AlphaZero MCTS variables
- `c_puct::Float32 = 1.0f0`: Exploration constant for AlphaZero MCTS.
- `alpha_dirichlet::Float32 = 0.3f0`: Alpha parameter for Dirichlet noise at root nodes.
- `epsilon_dirichlet::Float32 = 0.25f0`: Dirichlet noise weight.
- `tau::Float32 = 1.0f0`: Temperature parameter for action selection.
- `collapse_tau_move::Int = 30`: Action number after which to collapse ฯ (tau) to 0.

## NN Training variables
- `replay_buffer_size::Int = 500`: Size of the replay buffer. It's advised to express this
   as a multiple of the number of environments.
- `min_train_samples::Int = 1_000`: Minimum number of samples to begin training.
- `train_freq::Int = 500`: Frequency of neural network training. Note that this value
   represents total environment steps, so it's best to express it as a multiple of the
   number of environments.
- `adam_lr::Float32 = 3e-4`: Learning rate for the Adam optimizer.
- `weight_decay::Float32 = 0f0`: L2 regularization weight decay.
- `batch_size::Int = 50_000`: Batch size for training.
- `train_epochs::Int = 1`: Number of epochs per training step.

## Logging variables
- `train_logfile::String = ""`: File path to save training logs. Unused if equal to "".
- `eval_logfile::String = ""`: File path to save evaluation logs. Unused if equal to "".
- `tb_logdir::String = ""`: Directory for TensorBoard logs. Unused if equal to "".

## NN saving dir
- `nn_save_dir::String = ""`: Directory to save neural network model. Unused if equal to "".

## Evaluation variables
- `eval_fns::Vector{Function} = []`: Vector of evaluation functions.
- `eval_freq::Int = 0`: Frequency of evaluation during training. No evaluation function
   calls will take place if equal to 0.

## Total train steps
- `num_steps::Int = 1_500`: Total number of training steps. Note that this value
   represents total environment steps, so it's best to express it as a multiple of the
   number of environments.
"""
@kwdef struct TrainConfig
    # environment variables
    EnvCls::Type
    env_kwargs::Dict
    num_envs::Int

    # common MCTS variables
    use_gumbel_mcts::Bool = true
    num_simulations::Int = 64

    # Gumbel MCTS variables
    num_considered_actions::Int = 9
    mcts_value_scale::Float32 = 0.1f0
    mcts_max_visit_init::Int = 50

    # AlphaZero MCTS variables
    c_puct::Float32 = 1.0f0
    alpha_dirichlet::Float32 = 0.3f0
    epsilon_dirichlet::Float32 = 0.25f0
    tau::Float32 = 1.0f0
    collapse_tau_move::Int = 30

    # NN Training variables
    replay_buffer_size::Int = 1_500
    min_train_samples::Int = 500
    train_freq::Int = 50
    adam_lr::Float32 = 3e-4
    weight_decay::Float32 = 0f0
    batch_size::Int = 50_000
    train_epochs::Int = 1

    # Logging variables
    train_logfile::String = ""
    eval_logfile::String = ""
    tb_logdir::String = ""

    # NN saving dir
    nn_save_dir::String = ""

    # Evaluation variables
    eval_fns::Vector{Function} = []
    eval_freq::Int = 0

    # Total train steps
    num_steps::Int = 1_500
end

"""
    init_envs(config::TrainConfig, num_envs::Int, device::Device)

Initializes and returns arrays contraining the environments to be used for training and
the number-of-episode-steps counter for every environments. Both arrays are placed on the
`device` device.
"""
function init_envs(config::TrainConfig, num_envs::Int, device::Device)
    envs = DeviceArray(device)([config.EnvCls(; config.env_kwargs...) for _ in 1:num_envs])
    steps_counter = zeros(Int, device, num_envs)
    return envs, steps_counter
end

"""
    init_mcts_config(device::Device, nn::Net, config) where Net <: FluxNetwork

Initializes and returns an MCTS configuration object based on the `config` configuration
object. The `nn` neural network will be used in the environment oracle.

Note: This function can be dispatched on `config` object, as it can either be of type
`TrainConfig` or just a NamedTuple containing the important fields for the appropriate MCTS
configuration structure. The NamedTuple version will be used during evaluation to avoid
having to create an unnecessary `TrainConfig` object.
"""
function init_mcts_config(device::Device, nn::Net, config) where Net <: FluxNetwork
    oracle = neural_network_env_oracle(; nn)
    if config.use_gumbel_mcts
        return GumbelMctsConfig(;
            device,
            oracle,
            num_simulations=config.num_simulations,
            num_considered_actions=config.num_considered_actions,
            value_scale=config.mcts_value_scale,
            max_visit_init=config.mcts_max_visit_init
        )
    else
        return AlphaZeroMctsConfig(;
            device,
            oracle,
            num_simulations=config.num_simulations,
            c_puct=config.c_puct,
            alpha_dirichlet=config.alpha_dirichlet,
            epsilon_dirichlet=config.epsilon_dirichlet,
            tau=config.tau,
            collapse_tau_move=config.collapse_tau_move
        )
    end
end

"""
    save_nn(nn::Net, save_dir::String, step::Int, steps::Int) where Net <: FluxNetwork

Saves the `nn` neural network to the `save_dir` directory. The `step` and `steps` arguments
are used to create a filename for the saved model. Filenames will have padded 0's in front
to preserve the order of the files.

Notes:
- The `save_dir` directory is created if it doesn't exist.
- The model is transferred to the CPU before saving.
"""
function save_nn(nn::Net, save_dir::String, step::Int, steps::Int) where Net <: FluxNetwork
    !isdir(save_dir) && mkpath(save_dir)
    model_state = Flux.state(Flux.cpu(nn))
    step_str = lpad(step, length(string(steps)), "0")
    jldsave("$save_dir/model_$step_str.jld2"; model_state)
end

"""
    TrainExecutionTimes

Structure for storing execution times of important components of AlphaZeros's training loop.
"""
struct TrainExecutionTimes
    explore_times::Array{Float64}
    selection_times::Array{Float64}
    step_save_reset_times::Array{Float64}
    train_times::Array{Float64}
    eval_times::Array{Float64}
end

function TrainExecutionTimes(total_batch_steps)
    explore_times = zeros(Float64, total_batch_steps)
    selection_times = zeros(Float64, total_batch_steps)
    step_save_reset_times = zeros(Float64, total_batch_steps)
    train_times = zeros(Float64, total_batch_steps)
    eval_times = zeros(Float64, total_batch_steps)
    return TrainExecutionTimes(
        explore_times,
        selection_times,
        step_save_reset_times,
        train_times,
        eval_times
    )
end

function get_train_timestamps(execution_times::TrainExecutionTimes, config::TrainConfig)
    total_batch_steps = config.num_steps รท config.num_envs
    batch_eval_freq = config.eval_freq รท config.num_envs

    timestamps = zeros(Float64, total_batch_steps)
    train_timestamps = []
    for step in 1:total_batch_steps
        step_duration = execution_times.explore_times[step] +
            execution_times.selection_times[step] +
            execution_times.step_save_reset_times[step] +
            execution_times.train_times[step]
        timestamps[step] = step_duration + (step == 1 ? 0 : timestamps[step - 1])
        (step % batch_eval_freq == 0) && push!(train_timestamps, timestamps[step])
    end

    return train_timestamps
end

function print_execution_times(execution_times::TrainExecutionTimes)
    batch_steps = length(execution_times.explore_times)

    total_exp_time = round(sum(execution_times.explore_times), digits=4)
    avg_exp_time = round(total_exp_time / batch_steps, digits=4)

    total_select_time = round(sum(execution_times.selection_times), digits=4)
    avg_select_time = round(total_select_time / batch_steps, digits=4)

    total_ssr_time = round(sum(execution_times.step_save_reset_times), digits=4)
    avg_ssr_time = round(total_ssr_time / batch_steps, digits=4)

    train_times = [t for t in execution_times.train_times if t != zero(Float64)]
    total_train_time = round(sum(train_times), digits=4)
    avg_train_time = round(total_train_time / length(train_times), digits=4)

    eval_times = [t for t in execution_times.eval_times if t != zero(Float64)]
    total_eval_time = round(sum(eval_times), digits=4)
    avg_eval_time = round(total_eval_time / length(eval_times), digits=4)

    println("\n")
    println("Total explore time: $total_exp_time seconds.")
    println("Total selection time: $total_select_time seconds.")
    println("Total step-save-reset time: $total_ssr_time seconds.")
    println("Total train time: $total_train_time seconds.")
    println("Total eval time: $total_eval_time seconds.")
    println()
    println("Average explore time: $avg_exp_time seconds.")
    println("Average selection time: $avg_select_time seconds.")
    println("Average step-save-reset time: $avg_ssr_time seconds.")
    println("Average train time: $avg_train_time seconds.")
    println("Average eval time: $avg_eval_time seconds.")

    train_loop_time = total_exp_time + total_select_time + total_ssr_time + total_train_time
    train_loop_time = round(train_loop_time, digits=4)

    total_time = round(train_loop_time + total_eval_time, digits=4)
    println("\n")
    println("Total train loop time: $train_loop_time seconds.")
    println("Total time (train + eval): $total_time seconds.")
end

end
